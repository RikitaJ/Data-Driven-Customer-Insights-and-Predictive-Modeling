{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1rX6OGGxBp1"
      },
      "source": [
        "# Task 1\n",
        "\n",
        "---\n",
        "\n",
        "## Web scraping and analysis\n",
        "\n",
        "We will use a package called `BeautifulSoup` to collect the data from the web. O\n",
        "\n",
        "### Scraping data from Skytrax\n",
        "\n",
        "If you visit [https://www.airlinequality.com] you can see that there is a lot of data there. For this task, we are only interested in reviews related to British Airways and the Airline itself.\n",
        "\n",
        "If you navigate to this link: [https://www.airlinequality.com/airline-reviews/british-airways] you will see this data. Now, we can use `Python` and `BeautifulSoup` to collect all the links to the reviews and then to collect the text data on each of the individual review links."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtJdY0_hxBp5"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uc1tvYynxBp7",
        "outputId": "5e80e0ef-0df2-40e7-c37e-a40aa670e0a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page 1\n",
            "   ---> 100 total reviews\n",
            "Scraping page 2\n",
            "   ---> 200 total reviews\n",
            "Scraping page 3\n",
            "   ---> 300 total reviews\n",
            "Scraping page 4\n",
            "   ---> 400 total reviews\n",
            "Scraping page 5\n",
            "   ---> 500 total reviews\n",
            "Scraping page 6\n",
            "   ---> 600 total reviews\n",
            "Scraping page 7\n",
            "   ---> 700 total reviews\n",
            "Scraping page 8\n",
            "   ---> 800 total reviews\n",
            "Scraping page 9\n",
            "   ---> 900 total reviews\n",
            "Scraping page 10\n",
            "   ---> 1000 total reviews\n",
            "Scraping page 11\n",
            "   ---> 1100 total reviews\n",
            "Scraping page 12\n",
            "   ---> 1200 total reviews\n",
            "Scraping page 13\n",
            "   ---> 1300 total reviews\n",
            "Scraping page 14\n",
            "   ---> 1400 total reviews\n",
            "Scraping page 15\n",
            "   ---> 1500 total reviews\n",
            "Scraping page 16\n",
            "   ---> 1600 total reviews\n",
            "Scraping page 17\n",
            "   ---> 1700 total reviews\n",
            "Scraping page 18\n",
            "   ---> 1800 total reviews\n",
            "Scraping page 19\n",
            "   ---> 1900 total reviews\n",
            "Scraping page 20\n",
            "   ---> 2000 total reviews\n",
            "Scraping page 21\n",
            "   ---> 2100 total reviews\n",
            "Scraping page 22\n",
            "   ---> 2200 total reviews\n",
            "Scraping page 23\n",
            "   ---> 2300 total reviews\n",
            "Scraping page 24\n",
            "   ---> 2400 total reviews\n",
            "Scraping page 25\n",
            "   ---> 2500 total reviews\n",
            "Scraping page 26\n",
            "   ---> 2600 total reviews\n",
            "Scraping page 27\n",
            "   ---> 2700 total reviews\n",
            "Scraping page 28\n",
            "   ---> 2800 total reviews\n",
            "Scraping page 29\n",
            "   ---> 2900 total reviews\n",
            "Scraping page 30\n",
            "   ---> 3000 total reviews\n",
            "Scraping page 31\n",
            "   ---> 3100 total reviews\n",
            "Scraping page 32\n",
            "   ---> 3200 total reviews\n",
            "Scraping page 33\n",
            "   ---> 3300 total reviews\n",
            "Scraping page 34\n",
            "   ---> 3400 total reviews\n",
            "Scraping page 35\n",
            "   ---> 3500 total reviews\n",
            "Scraping page 36\n",
            "   ---> 3600 total reviews\n",
            "Scraping page 37\n",
            "   ---> 3700 total reviews\n",
            "Scraping page 38\n",
            "   ---> 3800 total reviews\n",
            "Scraping page 39\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 40\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 41\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 42\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 43\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 44\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 45\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 46\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 47\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 48\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 49\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 50\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 51\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 52\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 53\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 54\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 55\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 56\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 57\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 58\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 59\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 60\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 61\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 62\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 63\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 64\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 65\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 66\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 67\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 68\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 69\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 70\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 71\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 72\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 73\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 74\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 75\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 76\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 77\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 78\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 79\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 80\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 81\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 82\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 83\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 84\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 85\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 86\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 87\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 88\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 89\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 90\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 91\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 92\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 93\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 94\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 95\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 96\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 97\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 98\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 99\n",
            "   ---> 3809 total reviews\n",
            "Scraping page 100\n",
            "   ---> 3809 total reviews\n"
          ]
        }
      ],
      "source": [
        "base_url = \"https://www.airlinequality.com/airline-reviews/british-airways\"\n",
        "pages = 100\n",
        "page_size = 100\n",
        "\n",
        "reviews = []\n",
        "\n",
        "# for i in range(1, pages + 1):\n",
        "for i in range(1, pages + 1):\n",
        "\n",
        "    print(f\"Scraping page {i}\")\n",
        "\n",
        "    # Create URL to collect links from paginated data\n",
        "    url = f\"{base_url}/page/{i}/?sortby=post_date%3ADesc&pagesize={page_size}\"\n",
        "\n",
        "    # Collect HTML data from this page\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Parse content\n",
        "    content = response.content\n",
        "    parsed_content = BeautifulSoup(content, 'html.parser')\n",
        "    for para in parsed_content.find_all(\"div\", {\"class\": \"text_content\"}):\n",
        "        reviews.append(para.get_text())\n",
        "\n",
        "    print(f\"   ---> {len(reviews)} total reviews\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "7M_5XzrOxBp9",
        "outputId": "6c524179-7b07-4ba8-8c54-17df6f0abcc4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             reviews\n",
              "0  ✅ Trip Verified |  Flew British Airways on BA ...\n",
              "1  ✅ Trip Verified |  BA cancelled the flight fro...\n",
              "2  ✅ Trip Verified | I strongly advise everyone t...\n",
              "3  ✅ Trip Verified | My partner and I were on the...\n",
              "4  Not Verified |  We had a Premium Economy retur..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b030f87f-eeaa-4cdb-8cec-9309c8ebe247\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reviews</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>✅ Trip Verified |  Flew British Airways on BA ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>✅ Trip Verified |  BA cancelled the flight fro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>✅ Trip Verified | I strongly advise everyone t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>✅ Trip Verified | My partner and I were on the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Not Verified |  We had a Premium Economy retur...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b030f87f-eeaa-4cdb-8cec-9309c8ebe247')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b030f87f-eeaa-4cdb-8cec-9309c8ebe247 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b030f87f-eeaa-4cdb-8cec-9309c8ebe247');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7a283918-effd-4968-9388-55512c1f3f93\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7a283918-effd-4968-9388-55512c1f3f93')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7a283918-effd-4968-9388-55512c1f3f93 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 3809,\n  \"fields\": [\n    {\n      \"column\": \"reviews\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3804,\n        \"samples\": [\n          \"\\u2705 Verified Review |  London Heathrow to Lima. After my rave review of Norwegian to California, I find myself back on British Airways on my regular trip to South America. No delays on departure and well ordered boarding, but despite the smiles, British Airways staff just can't quite reach out to passengers as was the Norwegian experience. Tired, grey Boeing 777 interior doesn't help either. Okay the rudiments of a service are there but come on, who decided no alcoholic drinks served with dinner or replace the afternoon tea with a box of sickly snacks and half a tumbler of water, to later serve quite a tasty lunch with just coffee or tea ? Pokey TV screen but okay IFE programme choice. I suppose this is the best we can expect but with so called low cost airlines doing so much better, the main carriers deserve to lose ground.\",\n          \"\\u2705 Trip Verified |  Have not flown economy with BA for years, what a shock! Hard uncomfortable seats, small aged tv screens with limited inflight entertainment. Food and service appalling. 13hr plus flight, no socks, eye shades, ear plug amenity pack. And no antibacterial wipes offered. Wi-Fi advertised but not available on flight. Paid premium money for a budget airline, more akin to Easy Jet, RyanAir or JetStar. My last time travelling with BA after more than 18 years of flying with them. BA you really do need to buck up your service and become the flagship Service you used to be.\",\n          \"\\u2705 Trip Verified |  The crew BA656 on 6 June absolutely made this flight. Smooth journey with possibly the nicest crew I've ever had. Rebooking for September we were that impressed.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "df = pd.DataFrame()\n",
        "df[\"reviews\"] = reviews\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reviews[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "R9M4V_gFnyfo",
        "outputId": "b7bb2731-8d48-47fd-a714-ccc9668f8a28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'✅ Trip Verified |  BA cancelled the flight from Tokyo to LHR. I was booked on next day flight. There was another flight on the same day. I went to the desk, but the flight was full. BA in charge offers another flight through Hong Kong which would have been 26h flight time. I declined that, and asked to stay on the next day flight. To my dismay he cancelled the next day flight without telling me he did that. I think he was annoyed that I didn’t accept the offer after he spent sometime looking for. In fact I am the one who should be annoyed for cancelling my flight. I ended up flying another airline with downgrading. Poor service, and appalling behaviour. You expect better from BA.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create the 'data' directory if it doesn't exist\n",
        "if not os.path.exists(\"data\"):\n",
        "    os.makedirs(\"data\")\n",
        "\n",
        "df.to_csv(\"data/BA_reviews.csv\")"
      ],
      "metadata": {
        "id": "wWz6Xf3TyMQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhmBAGuAxBp-"
      },
      "source": [
        "\n",
        " The next thing that you should do is clean this data to remove any unnecessary text from each of the rows. For example, \"✅ Trip Verified\" can be removed from each row if it exists, as it's not relevant to what we want to investigate."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text Normalization"
      ],
      "metadata": {
        "id": "w_Xg_s4N4GjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1GG5COkz_2l",
        "outputId": "c47a2621-05a8-4bd2-a931-727a6834cb9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lX1Cxfmv0ZnN",
        "outputId": "c4353620-108c-422d-e62e-e0f9e76ec560"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Load stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to normalize text\n",
        "def normalize_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    # Tokenize\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    # Lemmatize\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    # Join tokens back to string\n",
        "    normalized_text = ' '.join(tokens)\n",
        "    return normalized_text"
      ],
      "metadata": {
        "id": "SmhBob890Zqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply text normalization to the desired column (replace 'reviews' with your actual text column name)\n",
        "df['normalized_text'] = df['reviews'].apply(normalize_text)\n",
        "\n",
        "# Display the first few rows of the normalized dataset\n",
        "print(df[['reviews', 'normalized_text']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gER8HWDe249t",
        "outputId": "3169729a-aa8b-42ee-8461-91a03646553e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             reviews  \\\n",
            "0  ✅ Trip Verified |  Flew British Airways on BA ...   \n",
            "1  ✅ Trip Verified |  BA cancelled the flight fro...   \n",
            "2  ✅ Trip Verified | I strongly advise everyone t...   \n",
            "3  ✅ Trip Verified | My partner and I were on the...   \n",
            "4  Not Verified |  We had a Premium Economy retur...   \n",
            "\n",
            "                                     normalized_text  \n",
            "0  trip verified flew british airway ba london he...  \n",
            "1  trip verified ba cancelled flight tokyo lhr bo...  \n",
            "2  trip verified strongly advise everyone never f...  \n",
            "3  trip verified partner ba return flight tampa g...  \n",
            "4  verified premium economy return flight los ang...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Remove Stop Words:"
      ],
      "metadata": {
        "id": "qXxqdZU98uNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Step 3: Download NLTK stop words list\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Step 5: Define a function to remove stop words from a given text\n",
        "def remove_stop_words(text):\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "# Step 6: Apply the function to the text column in your dataset\n",
        "# Assuming the text data is in a column named 'text'\n",
        "df['cleaned_text'] = df['normalized_text'].apply(remove_stop_words)\n",
        "\n",
        "# Optional: Inspect the cleaned dataset to ensure stop words are removed\n",
        "print(df[['normalized_text', 'cleaned_text']].head())\n",
        "\n",
        "# Step 7: Save the cleaned dataset (optional, replace 'cleaned_dataset.csv' with your desired file path)\n",
        "df.to_csv('cleaned_dataset.csv', index=False)\n",
        "\n",
        "print(\"Cleaned dataset saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyLbn2A136jm",
        "outputId": "cf916159-d652-4480-a800-fcdc7eaf0e36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                     normalized_text  \\\n",
            "0  trip verified flew british airway ba london he...   \n",
            "1  trip verified ba cancelled flight tokyo lhr bo...   \n",
            "2  trip verified strongly advise everyone never f...   \n",
            "3  trip verified partner ba return flight tampa g...   \n",
            "4  verified premium economy return flight los ang...   \n",
            "\n",
            "                                        cleaned_text  \n",
            "0  trip verified flew british airway ba london he...  \n",
            "1  trip verified ba cancelled flight tokyo lhr bo...  \n",
            "2  trip verified strongly advise everyone never f...  \n",
            "3  trip verified partner ba return flight tampa g...  \n",
            "4  verified premium economy return flight los ang...  \n",
            "Cleaned dataset saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenization"
      ],
      "metadata": {
        "id": "2xLJnFoy9sn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Ensure you have the necessary NLTK data files\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Step 4: Define a function to tokenize text\n",
        "def tokenize_text(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "# Step 5: Apply the tokenization function to the text column of your dataset\n",
        "# Replace 'text_column' with the actual name of the text column in your dataset\n",
        "df['tokenized_text'] = df['cleaned_text'].apply(tokenize_text)\n",
        "\n",
        "# Display the first few rows to see the tokenized text\n",
        "print(df.head())\n",
        "\n",
        "# Step 6: Save the tokenized dataset (optional, replace 'tokenized_dataset.csv' with your desired file path)\n",
        "df.to_csv('tokenized_dataset.csv', index=False)\n",
        "\n",
        "print(\"Tokenized dataset saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36L6utPq8sk3",
        "outputId": "1bafccab-6ac8-4f74-b5d3-59fdb523d537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             reviews  \\\n",
            "0  ✅ Trip Verified |  Flew British Airways on BA ...   \n",
            "1  ✅ Trip Verified |  BA cancelled the flight fro...   \n",
            "2  ✅ Trip Verified | I strongly advise everyone t...   \n",
            "3  ✅ Trip Verified | My partner and I were on the...   \n",
            "4  Not Verified |  We had a Premium Economy retur...   \n",
            "\n",
            "                                     normalized_text  \\\n",
            "0  trip verified flew british airway ba london he...   \n",
            "1  trip verified ba cancelled flight tokyo lhr bo...   \n",
            "2  trip verified strongly advise everyone never f...   \n",
            "3  trip verified partner ba return flight tampa g...   \n",
            "4  verified premium economy return flight los ang...   \n",
            "\n",
            "                                        cleaned_text  \\\n",
            "0  trip verified flew british airway ba london he...   \n",
            "1  trip verified ba cancelled flight tokyo lhr bo...   \n",
            "2  trip verified strongly advise everyone never f...   \n",
            "3  trip verified partner ba return flight tampa g...   \n",
            "4  verified premium economy return flight los ang...   \n",
            "\n",
            "                                      tokenized_text  \n",
            "0  [trip, verified, flew, british, airway, ba, lo...  \n",
            "1  [trip, verified, ba, cancelled, flight, tokyo,...  \n",
            "2  [trip, verified, strongly, advise, everyone, n...  \n",
            "3  [trip, verified, partner, ba, return, flight, ...  \n",
            "4  [verified, premium, economy, return, flight, l...  \n",
            "Tokenized dataset saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stemming and Lemmatization:"
      ],
      "metadata": {
        "id": "K3MPA8al91F-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "humtRpi891d_",
        "outputId": "5b989acb-847d-49f3-9e60-b77bb6474a1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.6.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.6.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load spaCy's English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define the stemmer\n",
        "stemmer = PorterStemmer()\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Spp-weK-M8G",
        "outputId": "ba9269cc-b6d2-4144-ba60-980c1b8051f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def stem_text(text):\n",
        "    tokens = text.split()\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens if token not in stop_words]\n",
        "    return ' '.join(stemmed_tokens)\n",
        "\n",
        "# Function to perform lemmatization\n",
        "def lemmatize_text(text):\n",
        "    doc = nlp(text)\n",
        "    lemmatized_tokens = [token.lemma_ for token in doc if token.text not in stop_words]\n",
        "    return ' '.join(lemmatized_tokens)"
      ],
      "metadata": {
        "id": "K2UzEafg-NE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply stemming\n",
        "df['stemmed_text'] = df['cleaned_text'].apply(stem_text)\n"
      ],
      "metadata": {
        "id": "4qTFMwux-Yf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply lemmatization\n",
        "df['lemmatized_text'] = df['stemmed_text'].apply(lemmatize_text)\n",
        "\n",
        "# Display the results\n",
        "print(df['lemmatized_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1L1urUtsliK_",
        "outputId": "1dff0c76-9fe7-49f3-fcbe-a359565d4c49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0       trip verifi fly british airway ba london heath...\n",
            "1       trip verifi ba cancel flight tokyo lhr book ne...\n",
            "2       trip verifi strongli advis everyon never fli b...\n",
            "3       trip verifi partner ba return flight tampa gat...\n",
            "4       verifi premium economi return flight lo angel ...\n",
            "                              ...                        \n",
            "3804    busi lhr bkk first tri back ba year fly mani a...\n",
            "3805    lhr ham purser address club passeng name board...\n",
            "3806    son work british airway urg fli british airway...\n",
            "3807    london citynew york jfk via shannon realli nic...\n",
            "3808    sinlhr ba b first class old aircraft seat priv...\n",
            "Name: lemmatized_text, Length: 3809, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Remove Special Characters and Extra Whitespace:\n",
        "\n",
        "Eliminate special characters, emojis, URLs, and excessive whitespace to clean the text further.\n"
      ],
      "metadata": {
        "id": "URpbagJvnavt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Step 4: Define a function to clean text data\n",
        "def clean_text(text):\n",
        "    # Remove special characters\n",
        "    text = re.sub(r'[^A-Za-z0-9\\s]+', '', text)\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# Step 5: Apply the cleaning function to the relevant columns (assuming text is in a column named 'text_column')\n",
        "df['lemmatized_text'] = df['lemmatized_text'].apply(clean_text)\n",
        "\n",
        "# Optional: Display the first few rows of the cleaned dataset to verify\n",
        "print(df['lemmatized_text'])\n",
        "\n",
        "# Step 6: Save the cleaned dataset (optional, replace 'cleaned_dataset.csv' with your desired file path)\n",
        "df.to_csv('cleaned_dataset.csv', index=False)\n",
        "\n",
        "print(\"Cleaned dataset saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4oS9NbulFOF",
        "outputId": "7d263ff2-5e16-445e-e7a9-914d44bb9ee5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0       trip verifi fly british airway ba london heath...\n",
            "1       trip verifi ba cancel flight tokyo lhr book ne...\n",
            "2       trip verifi strongli advis everyon never fli b...\n",
            "3       trip verifi partner ba return flight tampa gat...\n",
            "4       verifi premium economi return flight lo angel ...\n",
            "                              ...                        \n",
            "3804    busi lhr bkk first tri back ba year fly mani a...\n",
            "3805    lhr ham purser address club passeng name board...\n",
            "3806    son work british airway urg fli british airway...\n",
            "3807    london citynew york jfk via shannon realli nic...\n",
            "3808    sinlhr ba b first class old aircraft seat priv...\n",
            "Name: lemmatized_text, Length: 3809, dtype: object\n",
            "Cleaned dataset saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Handle Negations"
      ],
      "metadata": {
        "id": "XkAVaksfnueM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a list of common negation words\n",
        "negation_words = set([\"not\", \"no\", \"never\", \"n't\"])\n",
        "\n",
        "# Function to handle negations\n",
        "def handle_negations(text):\n",
        "    words = text.split()\n",
        "    negated_text = []\n",
        "    negation = False\n",
        "    for word in words:\n",
        "        word = word.lower()\n",
        "        # Check if the word is a negation\n",
        "        if word in negation_words:\n",
        "            negation = True\n",
        "            negated_text.append(word)\n",
        "        elif negation:\n",
        "            # Prefix the word with \"NOT_\" if negation is active\n",
        "            negated_text.append(\"NOT_\" + word)\n",
        "            negation = False\n",
        "        else:\n",
        "            negated_text.append(word)\n",
        "    return ' '.join(negated_text)\n",
        "\n",
        "# Apply the negation handling function to the DataFrame\n",
        "df['text_cleaned'] = df['lemmatized_text'].apply(handle_negations)\n",
        "\n",
        "# Display the cleaned text\n",
        "print(df[['lemmatized_text', 'text_cleaned']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWvSiSjaoalj",
        "outputId": "542b06b9-099a-4bbe-cb9e-0b6e0786216c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                        lemmatized_text  \\\n",
            "0     trip verifi fly british airway ba london heath...   \n",
            "1     trip verifi ba cancel flight tokyo lhr book ne...   \n",
            "2     trip verifi strongli advis everyon never fli b...   \n",
            "3     trip verifi partner ba return flight tampa gat...   \n",
            "4     verifi premium economi return flight lo angel ...   \n",
            "...                                                 ...   \n",
            "3804  busi lhr bkk first tri back ba year fly mani a...   \n",
            "3805  lhr ham purser address club passeng name board...   \n",
            "3806  son work british airway urg fli british airway...   \n",
            "3807  london citynew york jfk via shannon realli nic...   \n",
            "3808  sinlhr ba b first class old aircraft seat priv...   \n",
            "\n",
            "                                           text_cleaned  \n",
            "0     trip verifi fly british airway ba london heath...  \n",
            "1     trip verifi ba cancel flight tokyo lhr book ne...  \n",
            "2     trip verifi strongli advis everyon never NOT_f...  \n",
            "3     trip verifi partner ba return flight tampa gat...  \n",
            "4     verifi premium economi return flight lo angel ...  \n",
            "...                                                 ...  \n",
            "3804  busi lhr bkk first tri back ba year fly mani a...  \n",
            "3805  lhr ham purser address club passeng name board...  \n",
            "3806  son work british airway urg fli british airway...  \n",
            "3807  london citynew york jfk via shannon realli nic...  \n",
            "3808  sinlhr ba b first class old aircraft seat priv...  \n",
            "\n",
            "[3809 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Extraction:"
      ],
      "metadata": {
        "id": "uba5vHwNo0zY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bag of Words (BoW)"
      ],
      "metadata": {
        "id": "QrE1otcKpibi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Initialize CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform the text data\n",
        "bow_features = vectorizer.fit_transform(df['text_cleaned'])\n",
        "\n",
        "# Convert to DataFrame for better visualization\n",
        "bow_df = pd.DataFrame(bow_features.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "print(\"Bag of Words features:\")\n",
        "print(bow_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGUnQIvro1BV",
        "outputId": "d7643b7a-f852-45b7-c313-e862747e8ddf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words features:\n",
            "      aa  aacx  ab  aback  abandon  abandondon  abba  abbrevi  abc  abd  ...  \\\n",
            "0      0     0   0      0        0           0     0        0    0    0  ...   \n",
            "1      0     0   0      0        0           0     0        0    0    0  ...   \n",
            "2      0     0   0      0        0           0     0        0    0    0  ...   \n",
            "3      0     0   0      0        0           0     0        0    0    0  ...   \n",
            "4      0     0   0      0        0           0     0        0    0    0  ...   \n",
            "...   ..   ...  ..    ...      ...         ...   ...      ...  ...  ...  ...   \n",
            "3804   0     0   0      0        0           0     0        0    0    0  ...   \n",
            "3805   0     0   0      0        0           0     0        0    0    0  ...   \n",
            "3806   0     0   0      0        0           0     0        0    0    0  ...   \n",
            "3807   0     0   0      0        0           0     0        0    0    0  ...   \n",
            "3808   0     0   0      0        0           0     0        0    0    0  ...   \n",
            "\n",
            "      zombi  zone  zoo  zrh  zrhlhr  zrich  zuletzt  zum  zurich  \\\n",
            "0         0     0    0    0       0      0        0    0       0   \n",
            "1         0     0    0    0       0      0        0    0       0   \n",
            "2         0     0    0    0       0      0        0    0       0   \n",
            "3         0     0    0    0       0      0        0    0       0   \n",
            "4         0     0    0    0       0      0        0    0       0   \n",
            "...     ...   ...  ...  ...     ...    ...      ...  ...     ...   \n",
            "3804      0     0    0    0       0      0        0    0       0   \n",
            "3805      0     0    0    0       0      0        0    0       0   \n",
            "3806      0     0    0    0       0      0        0    0       0   \n",
            "3807      0     0    0    0       0      0        0    0       0   \n",
            "3808      0     0    0    0       0      0        0    0       0   \n",
            "\n",
            "      zusammenschluss  \n",
            "0                   0  \n",
            "1                   0  \n",
            "2                   0  \n",
            "3                   0  \n",
            "4                   0  \n",
            "...               ...  \n",
            "3804                0  \n",
            "3805                0  \n",
            "3806                0  \n",
            "3807                0  \n",
            "3808                0  \n",
            "\n",
            "[3809 rows x 10268 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Term Frequency-Inverse Document Frequency (TF-IDF)"
      ],
      "metadata": {
        "id": "8qoQwLXjqE9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the text data\n",
        "tfidf_features = tfidf_vectorizer.fit_transform(df['text_cleaned'])\n",
        "\n",
        "# Convert to DataFrame for better visualization\n",
        "tfidf_df = pd.DataFrame(tfidf_features.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "print(\"TF-IDF features:\")\n",
        "print(tfidf_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-JqMZlVqFr_",
        "outputId": "b8cb5489-5f5b-41e3-a071-216634e3b152"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF features:\n",
            "       aa  aacx   ab  aback  abandon  abandondon  abba  abbrevi  abc  abd  \\\n",
            "0     0.0   0.0  0.0    0.0      0.0         0.0   0.0      0.0  0.0  0.0   \n",
            "1     0.0   0.0  0.0    0.0      0.0         0.0   0.0      0.0  0.0  0.0   \n",
            "2     0.0   0.0  0.0    0.0      0.0         0.0   0.0      0.0  0.0  0.0   \n",
            "3     0.0   0.0  0.0    0.0      0.0         0.0   0.0      0.0  0.0  0.0   \n",
            "4     0.0   0.0  0.0    0.0      0.0         0.0   0.0      0.0  0.0  0.0   \n",
            "...   ...   ...  ...    ...      ...         ...   ...      ...  ...  ...   \n",
            "3804  0.0   0.0  0.0    0.0      0.0         0.0   0.0      0.0  0.0  0.0   \n",
            "3805  0.0   0.0  0.0    0.0      0.0         0.0   0.0      0.0  0.0  0.0   \n",
            "3806  0.0   0.0  0.0    0.0      0.0         0.0   0.0      0.0  0.0  0.0   \n",
            "3807  0.0   0.0  0.0    0.0      0.0         0.0   0.0      0.0  0.0  0.0   \n",
            "3808  0.0   0.0  0.0    0.0      0.0         0.0   0.0      0.0  0.0  0.0   \n",
            "\n",
            "      ...  zombi  zone  zoo  zrh  zrhlhr  zrich  zuletzt  zum  zurich  \\\n",
            "0     ...    0.0   0.0  0.0  0.0     0.0    0.0      0.0  0.0     0.0   \n",
            "1     ...    0.0   0.0  0.0  0.0     0.0    0.0      0.0  0.0     0.0   \n",
            "2     ...    0.0   0.0  0.0  0.0     0.0    0.0      0.0  0.0     0.0   \n",
            "3     ...    0.0   0.0  0.0  0.0     0.0    0.0      0.0  0.0     0.0   \n",
            "4     ...    0.0   0.0  0.0  0.0     0.0    0.0      0.0  0.0     0.0   \n",
            "...   ...    ...   ...  ...  ...     ...    ...      ...  ...     ...   \n",
            "3804  ...    0.0   0.0  0.0  0.0     0.0    0.0      0.0  0.0     0.0   \n",
            "3805  ...    0.0   0.0  0.0  0.0     0.0    0.0      0.0  0.0     0.0   \n",
            "3806  ...    0.0   0.0  0.0  0.0     0.0    0.0      0.0  0.0     0.0   \n",
            "3807  ...    0.0   0.0  0.0  0.0     0.0    0.0      0.0  0.0     0.0   \n",
            "3808  ...    0.0   0.0  0.0  0.0     0.0    0.0      0.0  0.0     0.0   \n",
            "\n",
            "      zusammenschluss  \n",
            "0                 0.0  \n",
            "1                 0.0  \n",
            "2                 0.0  \n",
            "3                 0.0  \n",
            "4                 0.0  \n",
            "...               ...  \n",
            "3804              0.0  \n",
            "3805              0.0  \n",
            "3806              0.0  \n",
            "3807              0.0  \n",
            "3808              0.0  \n",
            "\n",
            "[3809 rows x 10268 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sentiment Lexicons:"
      ],
      "metadata": {
        "id": "gZ_UOJQ6vVan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install afinn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkKS5ZbvsQtH",
        "outputId": "2f503084-7948-4398-bb98-48f73a87f97d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting afinn\n",
            "  Downloading afinn-0.1.tar.gz (52 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/52.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: afinn\n",
            "  Building wheel for afinn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for afinn: filename=afinn-0.1-py3-none-any.whl size=53430 sha256=aabd993ea876d27413ee0d067bd856bb634247d7bc06bef959b330abd93b67b9\n",
            "  Stored in directory: /root/.cache/pip/wheels/b0/05/90/43f79196199a138fb486902fceca30a2d1b5228e6d2db8eb90\n",
            "Successfully built afinn\n",
            "Installing collected packages: afinn\n",
            "Successfully installed afinn-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Step 3: Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Step 5: Text preprocessing\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text.lower())  # Convert to lowercase\n",
        "    # Remove stopwords and punctuation\n",
        "    cleaned_tokens = [token for token in tokens if token.isalnum() and token not in stop_words]\n",
        "    return ' '.join(cleaned_tokens)\n",
        "\n",
        "df['text_cleaned'] = df['text_cleaned'].apply(preprocess_text)\n",
        "\n",
        "# Step 6: Apply Sentiment Lexicons\n",
        "# Example: Using AFINN Lexicon\n",
        "from afinn import Afinn\n",
        "\n",
        "afinn = Afinn()\n",
        "\n",
        "def get_sentiment_score(text):\n",
        "    return afinn.score(text)\n",
        "\n",
        "df['sentiment_score'] = df['text_cleaned'].apply(get_sentiment_score)\n",
        "\n",
        "# Step 7: Save the cleaned dataset\n",
        "df.to_csv('cleaned_dataset_with_sentiment.csv', index=False)\n",
        "\n",
        "print(\"Cleaned dataset with sentiment scores saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsoadKEQrdEf",
        "outputId": "cbb43898-8fbc-4a22-fbee-c35d287f2966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned dataset with sentiment scores saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Split the dataset into training and testing sets"
      ],
      "metadata": {
        "id": "KiJLAzB5vCmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming your DataFrame is named df and it has a column 'sentiment' containing sentiment labels\n",
        "# 'sentiment' column could be categorical with labels like 'positive', 'negative', 'neutral'\n",
        "\n",
        "# Step 1: Split the dataset into features (X) and target (y)\n",
        "X = df.drop(columns=['sentiment_score'])  # Features\n",
        "y = df['sentiment_score']  # Target variable\n",
        "\n",
        "# Check for classes with only one member\n",
        "class_counts = y.value_counts()\n",
        "rare_classes = class_counts[class_counts == 1].index\n",
        "\n",
        "# Option 1: Remove samples with rare classes\n",
        "if not rare_classes.empty:\n",
        "    print(\"Removing samples with rare classes:\", rare_classes.tolist())\n",
        "    df_filtered = df[~y.isin(rare_classes)]\n",
        "    X = df_filtered.drop(columns=['sentiment_score'])\n",
        "    y = df_filtered['sentiment_score']\n",
        "\n",
        "# Option 2: If you can't remove samples, don't stratify\n",
        "if not rare_classes.empty:\n",
        "    print(\"Warning: Rare classes exist. Stratified splitting is not possible.\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "else:\n",
        "    # Step 2: Split the dataset into training and testing sets, maintaining the distribution of sentiments\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "\n",
        "# Step 3: Display the shape of the training and testing sets\n",
        "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
        "print(\"Testing set shape:\", X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQh4lSastxDV",
        "outputId": "3da9ec56-c10a-4fb1-ab28-0d9d063e3e3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing samples with rare classes: [53.0, -26.0, 54.0, 43.0, 35.0, 34.0, 30.0, 41.0]\n",
            "Warning: Rare classes exist. Stratified splitting is not possible.\n",
            "Training set shape: (3040, 7) (3040,)\n",
            "Testing set shape: (761, 7) (761,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# Download the VADER lexicon\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Initialize the VADER sentiment analyzer\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Function to get sentiment score\n",
        "def get_sentiment_score(text):\n",
        "    # Calculate sentiment scores\n",
        "    scores = sid.polarity_scores(text)\n",
        "    # Determine sentiment label based on compound score\n",
        "    if scores['compound'] >= 0.05:\n",
        "        return 'Positive'\n",
        "    elif scores['compound'] <= -0.05:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "# Apply sentiment analysis to each row in the dataset\n",
        "df['text_cleaned'] = df['text_cleaned'].apply(lambda x: get_sentiment_score(x))\n",
        "\n",
        "# Display the results\n",
        "print(df['text_cleaned'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrxt36OkxGr2",
        "outputId": "481688e2-e83f-49e3-c7d9-73dfc7e900a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0       Positive\n",
            "1       Negative\n",
            "2       Negative\n",
            "3       Positive\n",
            "4       Negative\n",
            "          ...   \n",
            "3804    Positive\n",
            "3805    Positive\n",
            "3806    Positive\n",
            "3807    Positive\n",
            "3808    Positive\n",
            "Name: text_cleaned, Length: 3809, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of positive, negative, and neutral sentiments\n",
        "sentiment_counts = df['text_cleaned'].value_counts()\n",
        "\n",
        "# Calculate the percentage of each sentiment\n",
        "total_counts = len(df['text_cleaned'])\n",
        "sentiment_percentages = (sentiment_counts / total_counts) * 100\n",
        "\n",
        "# Display the sentiment counts and percentages\n",
        "print(sentiment_counts)\n",
        "print(sentiment_percentages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ei1rLg2zc-Q",
        "outputId": "27312b05-8817-4e6e-89a4-f89b4e24d504"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text_cleaned\n",
            "Positive    2560\n",
            "Negative    1163\n",
            "Neutral       86\n",
            "Name: count, dtype: int64\n",
            "text_cleaned\n",
            "Positive    67.209241\n",
            "Negative    30.532948\n",
            "Neutral      2.257810\n",
            "Name: count, dtype: float64\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.13 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "4f7924c4c56b083e0e50eadfe7ef592a7a8ef70df33a0047f82280e6be1afe15"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}